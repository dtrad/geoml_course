{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "# 2D Fault Detection\n",
    "### Due date: December 13, 2021\n",
    "In this code, a neural network model will be trained with synthetic data as an image segmentation solution to detect faults on seismic sessions.\\\n",
    "Your work: \n",
    "\n",
    "* complete missing parts (in loading data)\n",
    "* write the network.\n",
    "* try predicting on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, loading some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading and preprocessing\n",
    "The data for this assignment is in D2L. It consist of a zip file with seismic images and labels (masks).\\\n",
    "The data are already divided in training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting image files locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the path to the files (read from D2L and upload to COLAB or copy to your system) \n",
    "MYPATH= ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "folder_train_x = MYPATH + \"/2D_train/seis/*.png\"\n",
    "folder_train_y = MYPATH + \"/2D_train/fault/*.png\"\n",
    "\n",
    "# Validation set\n",
    "folder_validation_x = ...\n",
    "folder_validation_y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining meta-data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (128, 128)\n",
    "batch_size = 20\n",
    "seed = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading images and masks:\n",
    "Using as example the code for the \n",
    "* folder_train_x, \\\n",
    "complete the reading, loading and preprocessing for:\n",
    "* folder_train_y, \n",
    "* folder_validation_x \n",
    "* folder_validation_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "xfiles = glob.glob(folder_train_x)\n",
    "yfiles = ...\n",
    "xfiles = np.sort(xfiles)\n",
    "yfiles = ...\n",
    "train_images = np.array([np.array(load_img(i, color_mode = \"grayscale\")) / 255 for i in xfiles])\n",
    "train_masks = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_images.shape, train_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "xfiles = ...\n",
    "yfiles = ...\n",
    "xfiles = ...\n",
    "yfiles = ...\n",
    "validation_images = ...\n",
    "validation_masks = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(validation_images.shape, validation_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping `numpy` arrays for the modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "train_images = train_images.reshape(-1,128,128,1)\n",
    "train_masks = train_masks.reshape(-1,128,128,1)\n",
    "\n",
    "# Validation set\n",
    "validation_images = validation_images.reshape(-1,128,128,1)\n",
    "validation_masks = validation_masks.reshape(-1,128,128,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.append(train_images, [np.fliplr(x) for x in train_images], axis=0)\n",
    "train_masks = np.append(train_masks, [np.fliplr(x) for x in train_masks], axis=0)\n",
    "train_images = np.append(train_images, [np.flipud(x) for x in train_images], axis=0)\n",
    "train_masks = np.append(train_masks, [np.flipud(x) for x in train_masks], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 10, figsize=(22,8))\n",
    "for i in range(10):\n",
    "    axs[0][i].imshow(train_images[i].squeeze(), cmap=\"Greys\")\n",
    "    axs[0][i].imshow(train_masks[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "    axs[1][i].imshow(train_images[int(len(train_images)/4 + i)].squeeze(), cmap=\"Greys\")\n",
    "    axs[1][i].imshow(train_masks[int(len(train_masks)/4 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "    axs[2][i].imshow(train_images[int(len(train_images)/2 + i)].squeeze(), cmap=\"Greys\")\n",
    "    axs[2][i].imshow(train_masks[int(len(train_masks)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "    axs[3][i].imshow(train_images[int(len(train_images)/4*3 + i)].squeeze(), cmap=\"Greys\")\n",
    "    axs[3][i].imshow(train_masks[int(len(train_masks)/4*3 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "fig.suptitle(\"Top row: original images, bottom 3 rows: augmented images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_images.shape, train_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "write your network. It can be a unet, or any other you would like to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = \"myunet\"\n",
    "model1 = unet()\n",
    "\n",
    "model1.compile(\n",
    "    loss = \"binary_crossentropy\", \n",
    "    optimizer = \"adam\",\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model:\n",
    "use the path below to save your models in unique directories for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "save_model_name = f\"./model/{model_type}_trained_v{version}.model\"\n",
    "save_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'accuracy', \n",
    "    mode = 'max',\n",
    "    patience = 15, \n",
    "    verbose = 1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    save_model_name, \n",
    "    monitor = 'accuracy', \n",
    "    mode = 'max',\n",
    "    save_best_only = True, \n",
    "    verbose = 1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'accuracy', \n",
    "    mode = 'max', \n",
    "    factor = 0.5, \n",
    "    patience = 7,\n",
    "    min_lr = 0.0001, \n",
    "    verbose = 1)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 20\n",
    "\n",
    "t_model1_start = time.time()\n",
    "history = model1.fit(train_images, train_masks,\n",
    "                     validation_data = (validation_images, validation_masks), \n",
    "                     epochs = epochs, \n",
    "                     batch_size = batch_size, \n",
    "                     callbacks = [early_stopping, model_checkpoint, reduce_lr],\n",
    "                     verbose = 1)\n",
    "t_model1_end = time.time()\n",
    "print(f\"Run time = {(t_model1_end-t_model1_start)/3600} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part, we load the model from disk and use it for prediction.\\\n",
    "If you have more than one model saved, choose the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "model_type = \"myunet\"\n",
    "model_name = f\"./model/{model_type}_trained_v{version}.model\"\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = model.predict(COMPLETE HERE...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = preds_valid.reshape(-1, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def plotImageTranspValid(file1, file2, file3, k, alpha1 = 0.2, alpha2 = 0.2):\n",
    "    fig, ax = plt.subplots(nrows=k, ncols=k, figsize=(18, 18))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            ind = randint(0,file1.shape[0]-1)\n",
    "            ax[i,j].imshow(file1[ind], cmap='Greys')\n",
    "            ax[i,j].imshow(file2[ind], cmap='Blues', alpha = alpha1)\n",
    "            ax[i,j].imshow(file3[ind], cmap='Reds', alpha = alpha2)\n",
    "            ax[i,j].set_axis_off()\n",
    "    fig.subplots_adjust(wspace = -0.15, hspace = 0.02)\n",
    "    plt.suptitle(\"Blue: faults, Red: prediction.\")\n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotImageTranspValid(validation_images, validation_masks, preds_valid, k = 8, alpha1 = 0.2, alpha2 = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a better threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 50)\n",
    "ious = np.array([iou_metric_batch(validation_masks, np.int32(preds_valid > threshold)) for threshold in thresholds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New predictions using best threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.int32(preds_valid > threshold_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(preds_valid[0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predictions[0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImageTransp(file1, file2, k, alpha = 0.2):\n",
    "    fig, ax = plt.subplots(nrows=k, ncols=k, figsize=(18, 18))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            ind = randint(0,file1.shape[0]-1)\n",
    "            ax[i,j].imshow(file1[ind], cmap='Greys')\n",
    "            ax[i,j].imshow(file2[ind], cmap='Purples', alpha = alpha)\n",
    "            ax[i,j].set_axis_off()\n",
    "    fig.subplots_adjust(wspace = -0.15, hspace = 0.02)\n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotImageTransp(validation_images, predictions, k = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
