{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with datasets in Tensorflow\n",
    "## The tf.keras.datasets vs the Tensorflow Datasets\n",
    "Introductory ML class on datasets and ML engineering\\\n",
    "\n",
    "Daniel Trad, using chatGPT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.keras.datasets module provides access to a number of public datasets as tf.data.Dataset objects, which are easy to use with tf.keras models. These datasets are small and well-understood, and are useful for testing and debugging.\n",
    "\n",
    "tfds (TensorFlow Datasets) is a collection of datasets ready to use with TensorFlow. It includes a wide range of datasets for various tasks such as object detection, language translation, and recommendation systems. The datasets provided by tfds are typically larger and more complex than those in tf.keras.datasets. They are also well-documented and include detailed information about the data, such as the number of classes, the format of the data, and how the data was collected and preprocessed. Additionally, tfds includes tools for loading, preprocessing, and manipulating the data, making it easier to work with large and complex datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of how you can create a deep neural network (DNN) in tf.keras for the MNIST dataset using tf.keras.datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 08:57:50.570562: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-06 08:57:51.742583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10226 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
      "2023-01-06 08:57:51.743118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 5859 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "2023-01-06 08:57:52.181692: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  29/1875 [..............................] - ETA: 3s - loss: 1.6431 - accuracy: 0.5603   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 08:57:53.570769: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 1ms/step - loss: 0.2265 - accuracy: 0.9334\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0968 - accuracy: 0.9707\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0674 - accuracy: 0.9786\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0508 - accuracy: 0.9838\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0392 - accuracy: 0.9870\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07693281024694443, 0.9782999753952026]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Build the model\n",
    "inputs = tf.keras.layers.Input(shape=(28, 28))\n",
    "x = Flatten()(inputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will create a simple DNN with two hidden layers (128 units each) and an output layer with 10 units, corresponding to the 10 classes in the MNIST dataset. The model is then compiled using the adam optimizer and the sparse_categorical_crossentropy loss function, and is trained using the training data for 5 epochs. Finally, the model is evaluated on the test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of how you can create a deep neural network (DNN) in tf.keras for the MNIST dataset using tfds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2371 - accuracy: 0.9301\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1002 - accuracy: 0.9693\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0682 - accuracy: 0.9792\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0483 - accuracy: 0.9851\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0368 - accuracy: 0.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40ba074700>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Load the MNIST dataset\n",
    "ds, info = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess(image, label):\n",
    "  image = tf.cast(image, tf.float32) / 255.0\n",
    "  return image, label\n",
    "\n",
    "ds = ds.map(preprocess)\n",
    "ds = ds.batch(32)\n",
    "ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Build the model\n",
    "inputs = tf.keras.layers.Input(shape=(28, 28))\n",
    "x = Flatten()(inputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(ds, epochs=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will create a simple DNN with two hidden layers (128 units each) and an output layer with 10 units, corresponding to the 10 classes in the MNIST dataset. The data is loaded using tfds.load and preprocessed by normalizing the pixel values and batching the data. The model is then compiled using the adam optimizer and the sparse_categorical_crossentropy loss function, and is trained using the training data for 5 epochs. Note that the test data is not used in this example.\n",
    "\n",
    "To use the test data to evaluate the model, you can use the ds_test dataset created in the same way as ds, and call model.evaluate on it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger datasets: machine learning engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different ways you could modify the example code to work with a larger dataset:\n",
    "\n",
    "Use a generator to load the data in batches: Instead of loading the entire dataset into memory at once, you can use a generator function to load the data in smaller batches, which can be more memory-efficient. You can then pass the generator to the fit function using the steps_per_epoch and validation_steps arguments.\n",
    "\n",
    "Use model checkpointing to save and restore the model weights: As the model trains, you can use the ModelCheckpoint callback to save the model weights to disk after each epoch. This way, if the training process is interrupted, you can restore the model weights from the most recent checkpoint and continue training from there.\n",
    "\n",
    "Use distributed training to train the model across multiple GPUs: If you have access to multiple GPUs, you can use TensorFlow's tf.distribute API to distribute the training process across multiple devices. This can significantly speed up training on large datasets.\n",
    "\n",
    "Preprocess the data in parallel: You can use the tf.data API to preprocess the data in parallel using multiple CPU cores. This can help to speed up the data loading and preprocessing steps, especially if the dataset is large.\n",
    "\n",
    "Use data augmentation to generate additional training data: If the dataset is small, you can use data augmentation techniques to generate additional training examples by applying random transformations to the existing data. This can help to improve the generalization performance of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how you could modify the code to use a data generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3011 - accuracy: 0.9134\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1453 - accuracy: 0.9567\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1080 - accuracy: 0.9677\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0884 - accuracy: 0.9734\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0735 - accuracy: 0.9768\n",
      "312/312 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0832531750202179, 0.9737580418586731]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a data generator\n",
    "def data_generator(x, y, batch_size=32):\n",
    "  while True:\n",
    "    for i in range(0, len(x), batch_size):\n",
    "      x_batch = x[i:i+batch_size]\n",
    "      y_batch = y[i:i+batch_size]\n",
    "      yield x_batch, y_batch\n",
    "\n",
    "# Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Create the data generators\n",
    "train_generator = data_generator(x_train, y_train)\n",
    "test_generator = data_generator(x_test, y_test)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_generator, steps_per_epoch=len(x_train) // 32, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(test_generator, steps=len(x_test) // 32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version of the code, we define a data_generator function that yields batches of data from the input arrays. We then create generator objects for the training and test data using this function. Finally, we pass the generator objects to the fit and evaluate functions using the steps_per_epoch and steps arguments, respectively. This allows the model to train and evaluate using the data in batches, rather than loading the entire dataset into memory at once."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use multiple GPUs with TensorFlow, you can use the tf.distribute.Strategy API. Here is an example of how you could modify the code to use 2 GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a data generator\n",
    "def data_generator(x, y, batch_size=32):\n",
    "  while True:\n",
    "    for i in range(0, len(x), batch_size):\n",
    "      x_batch = x[i:i+batch_size]\n",
    "      y_batch = y[i:i+batch_size]\n",
    "      yield x_batch, y_batch\n",
    "\n",
    "# Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Create the data generators\n",
    "train_generator = data_generator(x_train, y_train)\n",
    "test_generator = data_generator(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2023-01-06 10:26:10.297260: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_81611\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 8s 3ms/step - loss: 0.3060 - accuracy: 0.9115\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1530 - accuracy: 0.9555\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1148 - accuracy: 0.9651\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0946 - accuracy: 0.9707\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0785 - accuracy: 0.9757\n"
     ]
    }
   ],
   "source": [
    "# Use the MirroredStrategy to distribute the model across 2 GPUs\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Compile and train the model\n",
    "with strategy.scope():\n",
    "  # Define the model\n",
    "    model = createModel()\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    model.fit(train_generator, steps_per_epoch=len(x_train) // 32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 10:27:25.246835: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_101929\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 2s 2ms/step - loss: 0.0874 - accuracy: 0.9740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08742991834878922, 0.9740168452262878]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(test_generator, steps=len(x_test) // 32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version of the code, we use the tf.distribute.MirroredStrategy to distribute the model across 2 GPUs. The MirroredStrategy creates a copy of the model on each GPU and synchronizes the gradients and variables between the copies. To use the strategy, we first create a MirroredStrategy object, and then use the strategy.scope context manager to compile and train the model. This will automatically distribute the training process across the available GPUs. Note that you will need to have at least 2 GPUs available in order to run this code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50e0cab8bd62756c7fc66a63f77f1d7a5658785ded0a9e3406fe8e8350515108"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
