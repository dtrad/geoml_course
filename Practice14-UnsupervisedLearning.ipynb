{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Lecture 14: Unsupervised Learning\n",
    "In this lab we will look at unsupervised learning methods for classification and regression. \\\n",
    "Based on Chapter 9 from Aurelien Geron's book, Hands-on Machine Learning with Scikit-Learn Keras & Tensorflow.\\\n",
    "Original code examples from book in github [here](https://github.com/ageron/handson-ml2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/dtrad/geoml_course/blob/master/Practice14-UnsupervisedLearning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "sklearn.set_config(print_changed_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: clustering\n",
    "For the dataset below, use sklearn k-means and plot how it evolves with iterations 1 and 5.\\\n",
    "Use the example on the notebook to write your 2x2 plot.\\\n",
    "Visualize the movement of the centroids on the left column and the boundaries on the right column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.3,  1.8],\n",
    "     [-2.4,  2.8],\n",
    "     [-2.9,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.2, 0.4, 0.2])\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std, random_state=7)\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do unsupervised clustering we need to work without labels (without y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us remove the labels to do unsupervised clustering\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=None, s=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit method will calculate the centroids and labels, which we can access throught the kmeans variables.\\\n",
    "The predict method in \"fit_predict\" just gives us the same information and the labels_ variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "print('X shape', X.shape)\n",
    "print('y_pred shape', y_pred.shape)\n",
    "print('labels',kmeans.labels_)\n",
    "print('y_pred', y_pred)\n",
    "print('centers', kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of fit_predict will be the labels calculated inside k_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_ is y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a few plotting functions that we will need: \n",
    "1) plot data\n",
    "2) plot centroids\n",
    "3) plot decision boundaries\n",
    "\n",
    "Functions 1 and 2 are used in 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker='o', s=10, linewidths=10, color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=10, linewidths=15, color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), linewidths=1, colors='k')\n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the changes with iterations, we will use two clusters with 1 and 5 iterations.\\\n",
    "(the choice 1 to 5 is just to make changes more obvious, you can use 1 and 2 to see how much change occurs in 1 iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_iter1 = KMeans(n_clusters=5,init=\"random\", n_init=1, algorithm=\"full\", max_iter=1, random_state=1)\n",
    "kmeans_iter2 = KMeans(n_clusters=5,init=\"random\", n_init=1, algorithm=\"full\", max_iter=5, random_state=1)\n",
    "\n",
    "print(kmeans_iter1.fit(X))\n",
    "print(kmeans_iter2.fit(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmeans works by iterating through two key steps: \n",
    "1) assign centroids,\n",
    "2) assign labels. \n",
    "\n",
    "Each depends on the other, so we have to freeze one of them at each step and update the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting steps\n",
    "The following plots plots illustrate how the kmeans algorithm work. \n",
    "- initial cluster centers (random initialization)\n",
    "- calculate the distance between each sample and the centroids, assign the labels (and boundaries).\n",
    "- with the assigned labels calculate new centroids. \n",
    "- with the new centroids calculate the new labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(221)\n",
    "plot_data(X)\n",
    "plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\n",
    "plt.title('random centroids')\n",
    "plt.subplot(222)\n",
    "plot_decision_boundaries(kmeans_iter1, X)\n",
    "plt.title('same centroids, new labels')\n",
    "plt.subplot(223)\n",
    "plot_decision_boundaries(kmeans_iter1, X, show_centroids=False)\n",
    "plot_centroids(kmeans_iter2.cluster_centers_)\n",
    "plt.title('new centroids, same labels')\n",
    "plt.subplot(224)\n",
    "plot_decision_boundaries(kmeans_iter2, X)\n",
    "plt.title('same centroids, new labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Cluster best models\n",
    "Create clusters with different parameters and estimate their quality by using inertia.\\\n",
    "First try using different number of clusters.\\\n",
    "Then do the same with the different initialization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_v1 = KMeans(n_clusters=4, init=\"random\", n_init=1, algorithm=\"full\", random_state=42)\n",
    "kmeans_v2 = KMeans(n_clusters=5, init=\"random\", n_init=1, algorithm=\"full\", random_state=42)\n",
    "kmeans_v3 = KMeans(n_clusters=6, init=\"random\", n_init=1, algorithm=\"full\", random_state=42)\n",
    "kmeans_v1.fit(X)\n",
    "kmeans_v2.fit(X)\n",
    "kmeans_v3.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans_v1.inertia_)\n",
    "print(kmeans_v2.inertia_)\n",
    "print(kmeans_v3.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plot_decision_boundaries(kmeans_v1, X)\n",
    "plt.subplot(132)\n",
    "plot_decision_boundaries(kmeans_v2, X)\n",
    "plt.subplot(133)\n",
    "plot_decision_boundaries(kmeans_v3, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to test, we can create several clusters and plot their inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X) for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=np.arange(1,10)\n",
    "plt.plot(k,inertias);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inertia decays as we have more clusters, so it needs to be taken into account.\\\n",
    "The following metric is more accurate and clearly shows the decay of quality with larger clusters.\\\n",
    "Although it fails to notice superiority of k=5 over 4. An alternative is to use the silhouette coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[2:]]\n",
    "plt.plot(range(3, 10), silhouette_scores, \"bo-\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try now different initializations. Please try your own combinations until getting the smallest possible inertia.\n",
    "NOTE: The default initialization \"k-means++\" assigns preference to centroids that are far from the already selected centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "kmeans_v1 = KMeans(n_clusters=5, init=\"random\")\n",
    "kmeans_v2 = KMeans(n_clusters=5, init=good_init, n_init=1,algorithm=\"full\")\n",
    "kmeans_v3 = KMeans(n_clusters=5, init=\"k-means++\", n_init=10, random_state=42)\n",
    "\n",
    "\n",
    "print(kmeans_v1.fit(X))\n",
    "print(kmeans_v2.fit(X))\n",
    "print(kmeans_v3.fit(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans_v1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans_v1.inertia_)\n",
    "print(kmeans_v2.inertia_)\n",
    "print(kmeans_v3.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plot_decision_boundaries(kmeans_v1, X)\n",
    "plt.subplot(132)\n",
    "plot_decision_boundaries(kmeans_v2, X)\n",
    "plt.subplot(133)\n",
    "plot_decision_boundaries(kmeans_v3, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm parameter permits to define a faster algorithm like Elkhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit -n 10 KMeans(algorithm=\"full\").fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit -n 10 KMeans(algorithm=\"elkan\").fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Irregular clusters \n",
    "For the data below, try to fit a cluster without initial centers then giving the centers given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]])\n",
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "print('before rotation',X1[0])\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "print('after rotation',X1[0])\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]\n",
    "plt.scatter(X1[:,0],X1[:,1],c=y1)\n",
    "plt.scatter(X2[:,0],X2[:,1],c=y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_good = KMeans(n_clusters=3, init=good_init, n_init=1, random_state=42)\n",
    "kmeans_bad = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_good.fit(X)\n",
    "kmeans_bad.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3.2))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundaries(kmeans_good, X)\n",
    "plt.title(\"Inertia = {:.1f}\".format(kmeans_good.inertia_), fontsize=14)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n",
    "plt.title(\"Inertia = {:.1f}\".format(kmeans_bad.inertia_), fontsize=14);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Gaussian mixture model\n",
    "Using a Gaussian mixture model to separate the Iris dataset. Visually compare the predicted labels to the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gm=GaussianMixture(n_components=3)\n",
    "gm.fit(X) # notice we are not using y (it would supervised if we did)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=gm.predict(X)\n",
    "print(y_pred.shape)\n",
    "print(X.shape)\n",
    "print(data.target_names)\n",
    "print(data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[:150])\n",
    "print(y[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121);plt.scatter(X[:,2],X[:,3],c=y)\n",
    "plt.xlabel(\"Petal length\");plt.ylabel(\"Petal width\")\n",
    "plt.title(\"True\")\n",
    "plt.subplot(122);plt.scatter(X[:,2],X[:,3],c=y_pred)\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.title(\"Predicted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Gaussian Mixture Model for bloobs with different shapes\n",
    "Use a Gaussian mixture model to classify the moons data set with the parameters below.\\\n",
    "Check the model has converged and in how many iterations. Plot the decision boundaries. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make the problem harder by making elongated and circular blobs on top of each other. \\\n",
    "Let us build the model step by step for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "# let us make the blobs elongated by projecting them along a given matrix\n",
    "mapmatrix=np.array([[0.274, 0.95], [0.932, 0.598]])\n",
    "X1 = X1.dot(mapmatrix)\n",
    "plt.figure()\n",
    "plt.scatter(X1[:,0],X1[:,1])\n",
    "print('the program produces labels which we will not use y[:15]=',y1[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us make a third blob\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "# shift down by -8 to center it with the others\n",
    "X2 = X2 + [0, -4]\n",
    "y2 = y2 + 2; # need to shift to 2 because there are already 2 clusters.\n",
    "plt.figure()\n",
    "plt.scatter(X2[:,0],X2[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combine all together. We need to concatenate (by rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X1.shape)\n",
    "print(X2.shape)\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]\n",
    "print(X.shape,y.shape)\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1],c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to try unsupervised ML classification (we won't use the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gm=GaussianMixture(n_components=3)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianMixture class calculates the weights, means and variances for the Gaussian distributions.\\\n",
    "The weights assign relative importance to the 3 distributions. \\\n",
    "Notice from the covariances: 2 distributions are elongated along the right diagonal, the remaining distributions is circular\n",
    "### Questions:\n",
    "Could you tell which one is circular?\n",
    "Also, can you tell why we have weights of around 40%, 40%, 20%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('weights=',gm.weights_)\n",
    "print('\\n')\n",
    "print('means=\\n',gm.means_)\n",
    "print('\\n')\n",
    "print('covariances=\\n',gm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mapping matrix used to build the two elongated blobs\\n\\n',mapmatrix[:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us predict the classes for the given values and compare with the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=gm.predict(X)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121);plt.scatter(X[:,0],X[:,1],c=y);plt.title('True classes')\n",
    "plt.subplot(122);plt.scatter(X[:,0],X[:,1],c=y_pred);plt.title('predicted')\n",
    "print('converged?',gm.converged_)\n",
    "print('how many iterations?',gm.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now classify a new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew=np.array([0,0]).reshape(1,-1)\n",
    "print('class',gm.predict(Xnew))\n",
    "print('probabilities',gm.predict_proba(Xnew))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the PDF (probability density function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(gm.score_samples(Xnew))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the theory, integrating the PDF over all the data space will give us the full probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 100\n",
    "grid = np.arange(-10, 10, 1 / resolution)\n",
    "xx, yy = np.meshgrid(grid, grid)\n",
    "X_full = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "pdf = np.exp(gm.score_samples(X_full))\n",
    "pdf_probas = pdf * (1 / resolution) ** 2\n",
    "pdf_probas.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate a meshgrid to plot the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def plot_gaussian_mixture(clusterer, X, resolution=1000):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    # gmm.score_samples: Compute the weighted log probabilities for each sample.\n",
    "    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z,norm=LogNorm(vmin=1.0, vmax=30.0),\n",
    "                 levels=np.logspace(0, 2, 12))\n",
    "    plt.contour(xx, yy, Z, norm=LogNorm(vmin=1.0, vmax=30.0),\n",
    "                levels=np.logspace(0, 2, 12),\n",
    "                linewidths=1, colors='k')\n",
    "\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, linewidths=2, colors='r', linestyles='dashed')\n",
    "    \n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "    plot_centroids(clusterer.means_, clusterer.weights_)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gaussian_mixture(gm,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Bayesian Gaussian Mixture model\n",
    "Use Bayesian Gaussian Mixture to estimate the optimal number of clusters in the data set below.\\\n",
    "Plot the boundaries and examine the weights. Experiment with different number of clusters.\\\n",
    "Try full, spherical, tied and diag\n",
    "### Question:\n",
    "If you had to do an analogy with classical regularization, what model norm would you say the inversion is using? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = make_blobs(n_samples=1000, centers=((4, -4), (0, 0), (3,3), (1,-1)), random_state=42)\n",
    "print(X.shape)\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1],c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "bgm = BayesianGaussianMixture(n_components=10, max_iter=1000, covariance_type='full')\n",
    "print(bgm.fit(X))\n",
    "print('number of iterations',bgm.n_iter_)\n",
    "print('weight for the 10 components',np.round(bgm.weights_,2))\n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_gaussian_mixture(bgm, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian algorithm is behaving as a $\\ell_1$ model norm regularization, also known as minimum entropy in the Bayes context.\\\n",
    "Entropy is a minimum when the model coefficients are as few as possible, that is information is localized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting for dbscan result, we need to look at the cores centres, anomalies and non-cores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n",
    "    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "    core_mask[dbscan.core_sample_indices_] = True\n",
    "    anomalies_mask = dbscan.labels_ == -1\n",
    "    non_core_mask = ~(core_mask | anomalies_mask)\n",
    "\n",
    "    cores = dbscan.components_\n",
    "    anomalies = X[anomalies_mask]\n",
    "    non_cores = X[non_core_mask]\n",
    "    \n",
    "    plt.scatter(cores[:, 0], cores[:, 1],\n",
    "                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n",
    "    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])\n",
    "    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n",
    "                c=\"r\", marker=\"x\", s=100)\n",
    "    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=\".\")\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n",
    "    plt.title(\"eps={:.2f}, min_samples={}\".format(dbscan.eps, dbscan.min_samples), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "print(dbscan.fit(X))\n",
    "dbscan2 = DBSCAN(eps=0.2, min_samples=5)\n",
    "print(dbscan2.fit(X))\n",
    "\n",
    "plt.figure(figsize=(9, 3.2))\n",
    "plt.subplot(121)\n",
    "plot_dbscan(dbscan, X, size=100)\n",
    "plt.subplot(122)\n",
    "plot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining supervised with un-supervised.\n",
    "Here we apply KMeans to reduce the dimensionality of the dataset. \n",
    "We use a reduced version of MNIST(8x8 instead of 28x28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_digits, y_digits= load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_digits.shape, y_digits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try to apply a classification on the original dataset (64 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9622222222222222"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter=5000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the classification, we can try to cluster samples and apply LR classification on the clustered data.\\\n",
    "It is not obvious here but the pipeline will send to LR not the original images but the distance to the clusters.\\\n",
    "The problem would be to come up with an optimal number of clusters, which is non trivial as we saw.\\\n",
    "However, since the clustering is just a preprocessing, we can just search for the nclusters that produces the optimal score (for example, using CV).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline=Pipeline([(\"kmeans\",KMeans(n_clusters=99)),(\"log_reg\",LogisticRegression(max_iter=5000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9755555555555555"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_digits, y_digits = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labeled=50\n",
    "log_reg=LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X_train[:n_labeled],y_train[:n_labeled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=50\n",
    "kmeans=KMeans(n_clusters=k)\n",
    "X_digits_dist = kmeans.fit_transform(X_train)\n",
    "representative_digit_idx= np.argmin(X_digits_dist, axis=0)\n",
    "X_representative_digits=X_train[representative_digit_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,2))\n",
    "for index, X_representative_digit in enumerate(X_representative_digits):\n",
    "    plt.subplot(k//10,10,index+1)\n",
    "    plt.imshow(X_representative_digit.reshape(8,8),cmap=\"binary\",interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_representative_digits = y_train[representative_digit_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_representative_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_representative_digits, y_representative_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
    "log_reg.fit(X_train, y_train_propagated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_closest = 20\n",
    "\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
    "log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_train_partially_propagated == y_train[partially_propagated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tfgpu4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "50e0cab8bd62756c7fc66a63f77f1d7a5658785ded0a9e3406fe8e8350515108"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
